Create two separate Python programs for face recognition within a Django backend and React frontend setup.

Program 1 – Embedding Generator: Extract and store face embeddings from input images.

Program 2 – Recognition System: Detect and recognize multiple faces simultaneously in real time.

You may use any reliable modules or models (e.g., YOLO, RetinaFace, DeepSORT, face_recognition) to ensure high accuracy. Both programs should integrate smoothly with Django (backend) and React (frontend) without issues.
ChatGPT said:
Thought for 12s

Nice — I’ll give you two ready-to-run Python programs and the minimal Django + React glue so they work together:

Program 1 — Embedding Generator: a Django management command that scans an image folder (or uploaded images), detects faces (RetinaFace if available, otherwise face_recognition fallback), computes 128-d embeddings via face_recognition, and stores embeddings in a Django model (also explains alternate MongoDB option). It writes a compact encodings.npy + meta.json and/or stores per-person embeddings in the DB.

Program 2 — Recognition System: a realtime recognition service (FastAPI + WebSocket) that:

accepts camera frames from the React frontend (frames over WebSocket),

runs YOLOv8m for person detection (to reject non-person regions), then RetinaFace (or fallback) for accurate face boxes, then DeepSORT for stable multi-person tracking,

compares embeddings to the DB using face_recognition (vector distance) and returns recognition results (name, id, bbox, track_id) to the frontend over the same WebSocket.

optionally streams thumbnails (base64) and timestamps.

I’ll provide:

Django model + management command (embedding_generator) code.

Recognition service (recognition_server.py) with FastAPI + ultralytics YOLO + deep-sort-realtime integration + Django ORM access.

Minimal React component that captures webcam frames and sends to recognition server via WebSocket, and displays live recognized people.

requirements.txt, run instructions, and helpful notes (fallbacks, performance tips, GPU hints).

I made safe tradeoffs: where GPU-accelerated libraries are optional, I provide fallback CPU-friendly code; where several third-party packages exist I pick widely used ones (ultralytics/YOLOv8, face_recognition/dlib, deep-sort-realtime). If a package isn't available on your machine the script falls back to a simpler method.

Quick architecture summary (how pieces connect)

Django app holds FaceEmbedding model (person name, embedding vector, optional metadata).

Run the embedding generator management command to ingest enrollment images and populate DB and optional encodings.npy.

Start recognition_server.py (FastAPI). It imports Django settings to access FaceEmbedding.

Start React app. The React component captures frames and sends them via WebSocket to FastAPI.

FastAPI receives frames → detection (YOLO) → face detection (RetinaFace / fallback) → embedding → matching against stored embeddings → returns JSON results with names/ids/track_ids → React displays.

0 — requirements

Create requirements.txt:

# core
fastapi
uvicorn[standard]
websockets
numpy
pillow
opencv-python
dlib
face_recognition

# object detection + models
ultralytics            # YOLOv8

# face detector (optional but recommended)
retina-face-pytorch    # optional; fallback in code if not installed

# tracking
deep-sort-realtime

# Django (for embedding command & DB access)
Django>=3.2

# optional for MongoDB (if you want mongo instead of Django ORM)
pymongo

# utilities
python-multipart
aiofiles


Notes:

face_recognition requires dlib (on Windows you might need prebuilt wheels or Visual Studio Build Tools).

If you have GPU, install CUDA-enabled versions of torch before installing ultralytics (see PyTorch site).

If retina-face-pytorch name differs for your platform, see its docs; a fallback is provided.

1 — Django model

Add this to an app (e.g., attendance/models.py):

# attendance/models.py
from django.db import models
import numpy as np
import json

class FaceEmbedding(models.Model):
    """
    Stores a single embedding for a person (one row per registered face).
    If you want multiple embeddings per person, include person_id and allow multiple rows.
    """
    person_name = models.CharField(max_length=255)
    person_id = models.CharField(max_length=128, blank=True, null=True)  # optional unique id (roll no)
    embedding = models.BinaryField()  # store numpy array as bytes
    meta = models.JSONField(default=dict, blank=True)

    created_at = models.DateTimeField(auto_now_add=True)

    def set_embedding(self, arr: 'np.ndarray'):
        # store as bytes
        self.embedding = arr.astype('float32').tobytes()

    def get_embedding(self) -> 'np.ndarray':
        if not self.embedding:
            return None
        return np.frombuffer(self.embedding, dtype='float32')


Run migrations after adding model:

python manage.py makemigrations attendance
python manage.py migrate

2 — Program 1: Embedding generator (Django management command)

Create attendance/management/commands/generate_embeddings.py:

# attendance/management/commands/generate_embeddings.py
import os
import base64
import json
import numpy as np
from django.core.management.base import BaseCommand
from django.conf import settings

from attendance.models import FaceEmbedding

# Try retinaface if installed
try:
    from retinaface import RetinaFace
    HAVE_RETINA = True
except Exception:
    HAVE_RETINA = False

import face_recognition
from PIL import Image
import io

class Command(BaseCommand):
    help = "Scan a directory of enrollment images and store face embeddings."

    def add_arguments(self, parser):
        parser.add_argument('--images-dir', required=True, help='Root folder containing subfolders per person or images named with person id')
        parser.add_argument('--save-npy', action='store_true', help='Also save encodings to encodings.npy + meta.json')

    def handle(self, *args, **options):
        images_dir = options['images_dir']
        save_npy = options['save_npy']

        encodings = []
        metas = []

        for root, dirs, files in os.walk(images_dir):
            for fname in files:
                if fname.lower().endswith(('.jpg','.jpeg','.png')):
                    path = os.path.join(root, fname)
                    # Decide person_name: if inside subfolder, use folder name; else filename without ext
                    person_name = os.path.basename(root) if os.path.basename(root) != os.path.basename(images_dir) else os.path.splitext(fname)[0]

                    img = face_recognition.load_image_file(path)

                    # Face detection: try RetinaFace -> fallback to face_recognition
                    face_locations = []
                    if HAVE_RETINA:
                        try:
                            # RetinaFace returns dict of faces
                            faces = RetinaFace.extract_faces(img_path=path, align=False)
                            # Extract_faces returns crop images as PIL images; we'll convert to numpy and then to locate bounding boxes by comparing sizes
                            # Simpler: use face_recognition fallback for box coordinates if RetinaFace.extract_faces not returning boxes
                            # To keep robust, attempt face_recognition as final
                            face_locations = face_recognition.face_locations(img, model='cnn')
                        except Exception as e:
                            face_locations = face_recognition.face_locations(img, model='cnn')
                    else:
                        face_locations = face_recognition.face_locations(img, model='cnn')

                    if len(face_locations) == 0:
                        self.stdout.write(self.style.WARNING(f'No face found in {path}, skipping.'))
                        continue

                    # Use first face (if multiple, you can extend to store multiple per image)
                    face_encs = face_recognition.face_encodings(img, known_face_locations=face_locations)
                    if not face_encs:
                        self.stdout.write(self.style.WARNING(f'Failed to compute encoding for {path}'))
                        continue

                    embedding = np.array(face_encs[0], dtype='float32')

                    # Store in DB
                    fe = FaceEmbedding(person_name=person_name)
                    fe.set_embedding(embedding)
                    fe.meta = {'source_file': path}
                    fe.save()

                    encodings.append(embedding)
                    metas.append({'person_name': person_name, 'source': path})

                    self.stdout.write(self.style.SUCCESS(f'Stored embedding for {person_name} from {path}'))

        if save_npy and encodings:
            arr = np.stack(encodings, axis=0)
            np.save('encodings.npy', arr)
            with open('meta.json','w') as f:
                json.dump(metas, f, indent=2)
            self.stdout.write(self.style.SUCCESS(f'Saved encodings.npy and meta.json (count={len(encodings)})'))

        self.stdout.write(self.style.SUCCESS('Completed embedding generation.'))


Usage:

python manage.py generate_embeddings --images-dir /path/to/enrollments --save-npy


Notes:

Directory structure recommended:

enrollments/
  Alice/
    alice1.jpg
    alice2.jpg
  Bob/
    bob1.jpg


Each subfolder name becomes person_name. If you prefer one DB row per person, add logic to average embeddings per person (store multiple rows per image is simpler and more robust).

3 — Program 2: Recognition server (FastAPI, YOLOv8m, RetinaFace, DeepSORT)

Create recognition_server.py at your project root:

# recognition_server.py
import os
import cv2
import asyncio
import base64
import json
import numpy as np
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
import uvicorn
from typing import Dict, List
from datetime import datetime

# Setup Django environment so we can import the FaceEmbedding model
import django
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "your_django_project.settings")
django.setup()

from attendance.models import FaceEmbedding

# detection: YOLOv8 via ultralytics
from ultralytics import YOLO

# face detector (optional)
try:
    from retinaface import RetinaFace
    HAVE_RETINA = True
except Exception:
    HAVE_RETINA = False

import face_recognition

# deep-sort-realtime
from deep_sort_realtime.deepsort_tracker import DeepSort

app = FastAPI()

# Load YOLOv8 model (yolov8m recommended). It will automatically download weights the first time.
yolo = YOLO('yolov8m.pt')  # replace with 'yolov8n.pt' for smaller model if needed

# load embeddings from Django DB into memory for fast matching
def load_known_embeddings():
    rows = FaceEmbedding.objects.all()
    embeddings = []
    metas = []
    for r in rows:
        emb = r.get_embedding()
        if emb is None:
            continue
        embeddings.append(emb)
        metas.append({'person_name': r.person_name, 'person_id': r.person_id, 'db_id': r.pk})
    if embeddings:
        return np.stack(embeddings), metas
    else:
        return np.zeros((0,128), dtype='float32'), metas

KNOWN_EMBS, KNOWN_METAS = load_known_embeddings()
print("Loaded", KNOWN_EMBS.shape[0], "known embeddings")

# DeepSORT tracker
tracker = DeepSort(max_age=30)

# helper: compare embedding to known embeddings
def match_embedding(query_emb: np.ndarray, known_embs: np.ndarray, known_metas: List[Dict], threshold: float = 0.45):
    if known_embs.shape[0] == 0:
        return None, None
    # compute L2 distances
    diffs = known_embs - query_emb
    dists = np.linalg.norm(diffs, axis=1)
    idx = np.argmin(dists)
    return (known_metas[idx], float(dists[idx])) if dists[idx] < threshold else (None, float(dists[idx]))

# WebSocket manager to track connections (single frontend or multiple)
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    async def send_json(self, websocket: WebSocket, data):
        await websocket.send_json(data)

manager = ConnectionManager()

@app.websocket("/ws/recognize")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            msg = await websocket.receive_json()
            # Expect message: {'type':'frame', 'image': '<base64jpeg>'}
            if msg.get('type') == 'frame' and 'image' in msg:
                b64 = msg['image'].split(',',1)[-1]  # allow data URL or raw base64
                img_bytes = base64.b64decode(b64)
                npimg = np.frombuffer(img_bytes, dtype=np.uint8)
                frame = cv2.imdecode(npimg, cv2.IMREAD_COLOR)  # BGR
                # Process single frame (non-blocking)
                results = await asyncio.get_event_loop().run_in_executor(None, process_frame, frame)
                await manager.send_json(websocket, results)
            elif msg.get('type') == 'reload_db':
                global KNOWN_EMBS, KNOWN_METAS
                KNOWN_EMBS, KNOWN_METAS = load_known_embeddings()
                await manager.send_json(websocket, {'type':'reloaded', 'count': KNOWN_EMBS.shape[0]})
            else:
                await manager.send_json(websocket, {'type':'error', 'message': 'unsupported message type'})
    except WebSocketDisconnect:
        manager.disconnect(websocket)


def process_frame(frame_bgr):
    """
    Steps:
    1. Run YOLOv8 person detection to find candidate person boxes.
    2. For each person box, run face detection (RetinaFace preferred; fallback to face_recognition or opencv).
    3. For each face, compute embedding and match to known embeddings.
    4. Use DeepSORT to get stable tracking ids (we pass person bboxes to tracker).
    Returns JSON with detections.
    """
    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    h, w, _ = rgb.shape

    # 1) YOLO person detection
    yres = yolo.predict(rgb, imgsz=640, device='cpu', verbose=False)[0]  # returns a Results object
    detections = []
    for det in yres.boxes:
        cls = int(det.cls.cpu().numpy())
        conf = float(det.conf.cpu().numpy())
        # class 0 is person for COCO models
        if cls != 0 or conf < 0.4:
            continue
        x1, y1, x2, y2 = det.xyxy.cpu().numpy()[0]
        detections.append({'bbox':[int(x1), int(y1), int(x2-x1), int(y2-y1)], 'conf':conf})

    # Prepare deep-sort input format: list of [x,y,w,h,score]
    ds_input = []
    for d in detections:
        x,y,w_box,h_box = d['bbox']
        ds_input.append([x,y,w_box,h_box,d['conf']])

    tracks = tracker.update_tracks(ds_input, frame=frame_bgr)
    out = []
    for t in tracks:
        if not t.is_confirmed():
            continue
        track_id = t.track_id
        ltrb = t.to_ltrb()  # left, top, right, bottom
        x1,y1,x2,y2 = map(int, ltrb)
        # Crop area for face detection (shrink a bit)
        pad_x = int(0.05 * (x2-x1))
        pad_y = int(0.15 * (y2-y1))
        fx1 = max(0, x1 + pad_x)
        fy1 = max(0, y1 + pad_y)
        fx2 = min(w-1, x2 - pad_x)
        fy2 = min(h-1, y2 - pad_y)
        crop = rgb[fy1:fy2, fx1:fx2]
        face_box = None
        face_conf = 0.0
        if crop.size == 0:
            continue

        # face detection
        if HAVE_RETINA:
            try:
                # RetinaFace expects path or ndarray; using face_recognition fallback because RetinaFace python wrapper varies
                face_locs = face_recognition.face_locations(crop, model='cnn')
            except Exception:
                face_locs = face_recognition.face_locations(crop, model='cnn')
        else:
            face_locs = face_recognition.face_locations(crop, model='cnn')

        if face_locs:
            # If multiple faces, pick largest
            best = None
            best_area = 0
            for (top, right, bottom, left) in face_locs:
                area = (right-left)*(bottom-top)
                if area > best_area:
                    best_area = area
                    best = (top, right, bottom, left)
            if best is None:
                continue
            top,right,bottom,left = best
            # Map face box back to full image coords
            f_x1 = fx1 + left
            f_y1 = fy1 + top
            f_x2 = fx1 + right
            f_y2 = fy1 + bottom

            face_box = [int(f_x1), int(f_y1), int(f_x2-f_x1), int(f_y2-f_y1)]
            # encode face
            face_crop = rgb[f_y1:f_y2, f_x1:f_x2]
            if face_crop.size == 0:
                continue
            face_encs = face_recognition.face_encodings(face_crop)
            if not face_encs:
                matched_name = None
                matched_dist = None
            else:
                emb = np.array(face_encs[0], dtype='float32')
                meta, dist = match_embedding(emb, KNOWN_EMBS, KNOWN_METAS, threshold=0.48)
                matched_name = meta['person_name'] if meta else None
                matched_dist = dist
        else:
            continue

        out.append({
            'track_id': track_id,
            'person_bbox': [int(x1),int(y1),int(x2-x1),int(y2-y1)],
            'face_bbox': face_box,
            'name': matched_name,
            'distance': matched_dist,
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        })

    return {'type':'detections', 'detections': out}


Run server:

python recognition_server.py
# or
uvicorn recognition_server:app --host 0.0.0.0 --port 8000 --reload


Important notes:

yolo.predict might use the GPU if available and properly installed; otherwise CPU.

DeepSORT here is the deep-sort-realtime library; tune max_age and other parameters.

Matching threshold 0.45-0.6 should be tuned for your dataset and quality of camera (lower = stricter).

4 — React frontend (capture frames & show recognized people)

Create a single React component file (Tailwind friendly). It captures webcam frames, downscales them, encodes to JPEG base64 and sends to WS /ws/recognize.

// src/components/RealtimeRecognition.jsx
import React, { useRef, useEffect, useState } from "react";

export default function RealtimeRecognition() {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const wsRef = useRef(null);
  const [connected, setConnected] = useState(false);
  const [detections, setDetections] = useState([]);

  useEffect(() => {
    const constraints = { video: { width: 640, height: 480 } };
    navigator.mediaDevices.getUserMedia(constraints).then(stream => {
      if (videoRef.current) videoRef.current.srcObject = stream;
    }).catch(err => {
      console.error("getUserMedia error:", err);
    });

    wsRef.current = new WebSocket("ws://localhost:8000/ws/recognize");
    wsRef.current.onopen = () => setConnected(true);
    wsRef.current.onmessage = (ev) => {
      try {
        const msg = JSON.parse(ev.data);
        if (msg.type === "detections") {
          setDetections(msg.detections);
        } else if (msg.type === 'reloaded') {
          console.log('DB reloaded', msg.count);
        } else {
          // other messages
        }
      } catch (e) {
        console.error("Invalid WS message", e);
      }
    };
    wsRef.current.onclose = () => setConnected(false);

    const interval = setInterval(() => {
      sendFrame();
    }, 200); // send ~5 fps; tune as required

    return () => {
      clearInterval(interval);
      if (wsRef.current) wsRef.current.close();
      const tracks = videoRef.current?.srcObject?.getTracks();
      tracks?.forEach(t => t.stop());
    };
  }, []);

  const sendFrame = () => {
    if (!videoRef.current || !canvasRef.current || !wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext("2d");
    ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
    const dataUrl = canvas.toDataURL("image/jpeg", 0.7); // quality 0.7
    const msg = { type: 'frame', image: dataUrl };
    wsRef.current.send(JSON.stringify(msg));
  };

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-2">Realtime Recognition</h2>
      <div className="flex gap-4">
        <div>
          <video ref={videoRef} autoPlay muted width={640} height={480} className="border rounded" />
          <canvas ref={canvasRef} width={640} height={480} style={{display:'none'}} />
          <div>WebSocket status: {connected ? 'Connected' : 'Disconnected'}</div>
        </div>
        <div className="w-80">
          <h3 className="font-semibold">Detections</h3>
          {detections.length === 0 && <div>No detections yet</div>}
          <ul>
            {detections.map(d => (
              <li key={d.track_id} className="border rounded p-2 mb-2">
                <div><strong>Track:</strong> {d.track_id}</div>
                <div><strong>Name:</strong> {d.name ?? 'Unknown'}</div>
                <div><strong>Face bbox:</strong> {d.face_bbox?.join(', ')}</div>
                <div><small>{d.timestamp}</small></div>
              </li>
            ))}
          </ul>
        </div>
      </div>
    </div>
  );
}


Notes:

This is intentionally simple: it sends a frame every 200ms (~5fps). If bandwidth or CPU is an issue make it less frequent.

You can draw bounding boxes client-side if you send back bbox coords (the server sends face_bbox and person_bbox).

Secure your WebSocket when deploying (wss + auth tokens).

5 — Helpful run & deployment steps

Install requirements (or create a venv):

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt


Prepare Django:

Add attendance app to INSTALLED_APPS.

Add FaceEmbedding model and run migrations.

Place enrollment images and run:

python manage.py generate_embeddings --images-dir /path/to/enrollments --save-npy


Start recognition server:

uvicorn recognition_server:app --host 0.0.0.0 --port 8000 --reload


Start React app, open page with RealtimeRecognition component. Make sure the WS URL matches host/port.

6 — Tuning suggestions & tips

Threshold tuning: L2 threshold for face_recognition embeddings depends on camera quality. Typical good values: 0.45–0.6. Calibrate with a validation set.

Use multiple embeddings per person: store several images per person (and compare to all) — matches become more robust.

Use GPU: YOLOv8 is orders of magnitude faster with CUDA. If you have NVidia GPU, install proper torch before ultralytics.

Frame rate vs accuracy: sending fewer frames reduces CPU load. You can also run detection only every N frames and run tracking on intermediate frames.

Security: Add authentication for WebSocket (token) and rate-limit connections for production.

Resilience: add DB reload endpoint or a file watch to refresh known embeddings without restarting server. I included reload_db message handler.

7 — Alternative (MongoDB) & scaling notes

If you prefer MongoDB (you mentioned it previously), replace the Django model + saving with Mongo (store embeddings as arrays or binary). The recognition server can query the embeddings collection and cache them in memory.

For many persons (thousands), use an approximate nearest neighbor library (FAISS or Annoy) for matching instead of linear scan.

8 — Caveats & final notes

Face models (dlib / face_recognition) can be slower on CPU; RetinaFace + a light face embedding model can be faster if you adopt other embedding backbones (MobileFaceNet, ArcFace). I've chosen face_recognition for wide familiarity and ease.

The code aims for clarity and being runnable locally. Production deployment (multiple camera streams, HTTPS/WSS, cluster autoscaling) requires additional engineering.

If you want, I can:

convert recognition server to use WebRTC for low-latency streaming,

provide a Django REST API + Channels version (so everything runs strictly inside Django),

implement FAISS-based nearest-neighbor for very large datasets,

or produce a Dockerfile and docker-compose for the entire stack.